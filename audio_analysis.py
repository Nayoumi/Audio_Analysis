# -*- coding: utf-8 -*-
"""Audio_Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g3vl9xGHREi3BHlhXgdzFUtelB-raQDM

###**Corswara audio analysis**

###Importing required libraries
"""

import json
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow.keras as keras
import tensorflow as tf
#tf.compat.v1.enable_eager_execution()

"""###Downloading the data set from git hub"""

!git clone https://github.com/iiscleap/Coswara-Data.git

"""### Extracting the zipped files using extract.py file"""

import os
import sys
import subprocess
import numpy as np
import glob
import json
import pandas as pd

'''
This script creates a folder "Extracted_data" inside which it extracts all the wav files in the directories date-wise
'''

coswara_data_dir = os.path.abspath("/content/Coswara-Data") # Local Path of iiscleap/Coswara-Data Repo
extracted_data_dir = os.path.join(coswara_data_dir, 'Extracted_data')  

if not os.path.exists(coswara_data_dir):
    raise("Check the Coswara dataset directory!")

if not os.path.exists(extracted_data_dir):
    os.makedirs(extracted_data_dir) # Creates the Extracted_data folder if it doesn't exist

dirs_extracted = set(map(os.path.basename,glob.glob('{}/202*'.format(extracted_data_dir))))
dirs_all = set(map(os.path.basename,glob.glob('{}/202*'.format(coswara_data_dir))))

dirs_to_extract = list(set(dirs_all) - dirs_extracted)

for d in dirs_to_extract:
    p = subprocess.Popen('cat {}/{}/*.tar.gz.* |tar -xvz -C {}/'.format(coswara_data_dir, d, extracted_data_dir), shell=True)
    p.wait()


print("Extraction process complete!")

"""###Reading the csv file to analyse the data"""

metadata_file = pd.read_csv("/content/Coswara-Data/combined_data.csv")

metadata_file.head()

"""### Displaying the information of the each column,nullvalues,numer of rows,Datatype"""

metadata_file.info()

"""### unique_values in the data for each column"""

metadata_file.nunique()

"""###Count of males,female,other"""

metadata_file.g.value_counts()

"""###Covid status number of members for each category"""

metadata_file.covid_status.value_counts()

"""### smokers are present or not"""

metadata_file.smoker.value_counts()

"""### For cold column  only true values are present in data set"""

metadata_file.cold.value_counts()

"""### For fever column true values are present"""

metadata_file.fever.value_counts()

"""###cough column is having only true values"""

metadata_file.cough.value_counts()

"""### one the json file data of a patient"""

json_file = pd.read_json("/content/Coswara-Data/Extracted_data/20200504/1fMS9159tNOgEbgTXBoUpBXQUIr1/metadata.json",typ="series")

json_file

"""###using librosa to read and convert the audio signals into data"""

### Let's read a sample audio using librosa
import librosa
audio_file_path='/content/Coswara-Data/Extracted_data/20200415/03TmwzsdEBVEh35MRMbC9d0NnfI3/cough-heavy.wav'
librosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path)

print(librosa_audio_data) ### Data of the audio signal

### Lets plot the librosa audio data
import matplotlib.pyplot as plt
# Original audio with 1 channel 
plt.figure(figsize=(12, 4))
plt.plot(librosa_audio_data)

"""###mfcc data of the audio file"""

mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=40)
print(mfccs.shape)

mfccs

"""### Converting the folder data to classes,lables, mfccs and writing those into json file"""

import json
import os
import math
import librosa

DATASET_PATH = "/content/Coswara-Data/Extracted_data/20200505/0HIgO2EhOOW1msCbEw1kC8Qsx6D3"
JSON_PATH = "data_10.json"
SAMPLE_RATE = 22050
TRACK_DURATION = 20 # measured in seconds
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION
def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):
    #Extracts MFCCs from music dataset and saves them into a json file along witgh genre labels.

    # dictionary to store mapping, labels, and MFCCs
    data = {
        "mapping": [],
        "labels": [],
        "mfcc": []
    }


    # loop through all genre sub-folder
    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)

    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):
          #print(dirpath)

        # ensure we're processing a genre sub-folder level
        #if dirpath is not dataset_path:
          for f in filenames:
            #print(f)
            if (f != "metadata.json"):
              semantic_label = f.split("/")[-1]
              semantic_label = semantic_label.split(".wav")[0]
              data["mapping"].append(semantic_label)
              print("\nProcessing: {}".format(semantic_label))
              
              file_path = os.path.join(dirpath, f)
              signal, sample_rate = librosa.load(file_path, sr=librosa.get_samplerate(file_path))

              
              if(len(signal)!=0):
                #print(file_path)
                for d in range(num_segments):
                  start = samples_per_segment * d
                  finish = start + samples_per_segment
                  # extract mfcc
                  mfcc = librosa.feature.mfcc(signal, librosa.get_samplerate(file_path), n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)
                  mfcc = mfcc.T
                  data["mfcc"].append(mfcc.tolist())
                  data["labels"].append(d)
                  print("{}, segment:{}".format(file_path,d))
                    
   # save MFCCs to json file
    with open(json_path, "w") as fp:
        json.dump(data, fp, indent=4)
if __name__ == "__main__":
    save_mfcc(DATASET_PATH, JSON_PATH, num_segments=9)

"""### reading the downloaded json file to train the model according to labels"""

DATA_PATH = "/content/data_10.json"

def load_data(data_path):
    """Loads training dataset from json file.
        :param data_path (str): Path to json file containing data
        :return X (ndarray): Inputs
        :return y (ndarray): Targets
    """

    with open(data_path, "r") as fp:
        data = json.load(fp)
        print(len(data["mfcc"]))

    # convert lists to numpy arrays
    
    X =np.array(data["mfcc"])
    y = np.array(data["labels"])
    #X =tf.convert_to_tensor(X,dtype=object
    #X=list(flatten(X))
    print("Data succesfully loaded!")

    return  X, y
load_data(DATA_PATH)

"""###Spliting the data into train and tet data,building the deep learning mode
1. input layer
2.3 Dense layers(hidden layers)
3.output layer
"""

if __name__ == "__main__":

    # load data
    lst=[]
    X, y = load_data(DATA_PATH)
    X=np.array(X).reshape(-1,1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    # build network topology
    model = keras.Sequential([

        # input layer
        keras.layers.Flatten(input_shape=(8,8)),

        # 1st dense layer
        keras.layers.Dense(512, activation='relu'),

        # 2nd dense layer
        keras.layers.Dense(256, activation='relu'),

        # 3rd dense layer
        keras.layers.Dense(64, activation='relu'),

        # output layer
        keras.layers.Dense(9, activation='softmax')
    ])

X_train.shape

"""###Compiling the data and summary of the data"""

optimiser = keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=optimiser,loss='sparse_categorical_crossentropy',metrics=['accuracy'])

model.summary()

"""### Converting labels to tensor data"""

#X_train = tf.convert_to_tensor(X_train)
y_train = tf.convert_to_tensor(y_train)

print(y_train.shape)

"""#fitting the xtrain and ytrain modrl to find the accuracy of the model"""

history = model.fit(X_train,y_train ,validation_data=(X_test,y_test) batch_size = 4,epochs=100)